{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "image_captioning.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "K2s1A9eLRPEj"
      },
      "source": [
        "##### Copyright 2018 The TensorFlow Authors.\n",
        "*https://www.tensorflow.org/tutorials/text/image_captioning\n",
        "\n",
        "#### Traducido para Samsung DesArrolladoras.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab_type": "code",
        "id": "VRLVEKiTEn04",
        "colab": {}
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Cffg2i257iMS"
      },
      "source": [
        "# Image captioning con atención visual\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QASbY_HGo4Lq"
      },
      "source": [
        "Dada una imagen como la del ejemplo a continuación, nuestro objetivo es generar una leyenda como \"un surfista montando en una ola\".\n",
        "\n",
        "![Man Surfing](https://tensorflow.org/images/surf.jpg)\n",
        "\n",
        "*[Image Source](https://commons.wikimedia.org/wiki/Surfing#/media/File:Surfing_in_Hawaii.jpg); License: Public Domain*\n",
        "\n",
        "Para lograr esto, utilizamos un modelo basado en la atención, que nos permite ver en qué partes de la imagen se enfoca el modelo a medida que genera un título.\n",
        "\n",
        "![Prediction](https://tensorflow.org/images/imcap_prediction.png)\n",
        "\n",
        "El modelo de arquitectura es similar al del artículo: [Show, Attend and Tell: Neural Image Caption Generation with Visual Attention](https://arxiv.org/abs/1502.03044).\n",
        "\n",
        "Este cuaderno es un ejemplo de principio a fin. Cuando ejecutas el cuaderno, descarga el dataset [MS-COCO](http://cocodataset.org/#home) , preprocesas y almacenas en caché un subconjunto de imágenes usando Inception V3, entrenas un modelo codificador-decodificador y generas subtítulos en nuevas imágenes usando el modelo entrenado.\n",
        "\n",
        "En este ejemplo, entrenarás un modelo en una cantidad relativamente pequeña de datos: los primeros 30,000 subtítulos para aproximadamente 20,000 imágenes (ya que hay múltiples subtítulos por imagen en el conjunto de datos).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "U8l4RJ0XRPEm",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Generarás plots de atención para ver en qué partes de una imagen\n",
        "# se enfoca nuestro modelo durante el proceso de los subtítulos\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Scikit-learn incluye muchas herramientas útiles\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "from glob import glob\n",
        "from PIL import Image\n",
        "import pickle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "b6qbGw8MRPE5"
      },
      "source": [
        "## Descarga y preparación del dataset MS-COCO \n",
        "\n",
        "Usarás [MS-COCO dataset](http://cocodataset.org/#home) para entrenar al modelo. El conjunto de datos contiene más de 82,000 imágenes, cada una de las cuales tiene al menos 5 anotaciones de subtítulos diferentes. El siguiente código descarga y extrae el conjunto de datos automáticamente.\n",
        "\n",
        "**Cuidado: descarga de archivo muy grande**. Usarás el conjunto de entrenamiento, que es un archivo de 13GB.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "krQuPYTtRPE7",
        "colab": {}
      },
      "source": [
        "# Descargar archivos de anotaciones de subtítulos\n",
        "annotation_folder = '/annotations/'\n",
        "if not os.path.exists(os.path.abspath('.') + annotation_folder):\n",
        "  annotation_zip = tf.keras.utils.get_file('captions.zip',\n",
        "                                          cache_subdir=os.path.abspath('.'),\n",
        "                                          origin = 'http://images.cocodataset.org/annotations/annotations_trainval2014.zip',\n",
        "                                          extract = True)\n",
        "  annotation_file = os.path.dirname(annotation_zip)+'/annotations/captions_train2014.json'\n",
        "  os.remove(annotation_zip)\n",
        "\n",
        "# Descarga archivo de imágenes\n",
        "image_folder = '/train2014/'\n",
        "if not os.path.exists(os.path.abspath('.') + image_folder):\n",
        "  image_zip = tf.keras.utils.get_file('train2014.zip',\n",
        "                                      cache_subdir=os.path.abspath('.'),\n",
        "                                      origin = 'http://images.cocodataset.org/zips/train2014.zip',\n",
        "                                      extract = True)\n",
        "  PATH = os.path.dirname(image_zip) + image_folder\n",
        "  os.remove(image_zip)\n",
        "else:\n",
        "  PATH = os.path.abspath('.') + image_folder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aANEzb5WwSzg"
      },
      "source": [
        "## Opcional: limitar el tamaño del conjunto de entrenamiento\n",
        "Para acelerar el entrenamiento para este tutorial, usarás un subconjunto de 30,000 subtítulos y sus imágenes correspondientes para entrenar a nuestro modelo. Elegir usar más datos daría como resultado una mejor calidad de subtítulos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4G3b8x8_RPFD",
        "colab": {}
      },
      "source": [
        "# Leemos el archivo json \n",
        "with open(annotation_file, 'r') as f:\n",
        "    annotations = json.load(f)\n",
        "\n",
        "# Almacenamos los subtítulos y los nombres de las imágenes en vectores \n",
        "all_captions = []\n",
        "all_img_name_vector = []\n",
        "\n",
        "for annot in annotations['annotations']:\n",
        "    caption = '<start> ' + annot['caption'] + ' <end>'\n",
        "    image_id = annot['image_id']\n",
        "    full_coco_image_path = PATH + 'COCO_train2014_' + '%012d.jpg' % (image_id)\n",
        "\n",
        "    all_img_name_vector.append(full_coco_image_path)\n",
        "    all_captions.append(caption)\n",
        "\n",
        "# Mezclamos los subtítulos y los nombres de imagen juntos\n",
        "# Establecemos un estado aleatorio\n",
        "train_captions, img_name_vector = shuffle(all_captions,\n",
        "                                          all_img_name_vector,\n",
        "                                          random_state=1)\n",
        "\n",
        "# Seleccionamos los primeros 30000 subtítulos del conjunto aleatorio\n",
        "num_examples = 30000\n",
        "train_captions = train_captions[:num_examples]\n",
        "img_name_vector = img_name_vector[:num_examples]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mPBMgK34RPFL",
        "colab": {}
      },
      "source": [
        "len(train_captions), len(all_captions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8cSW4u-ORPFQ"
      },
      "source": [
        "## Preprocesar las imágenes empleando el InceptionV3\n",
        "\n",
        "A continuación, utilizarás InceptionV3 (que está entrenado previamente en Imagenet) para clasificar cada imagen. Extraerás entidades de la última capa convolucional.\n",
        "\n",
        "Primero, convertirás las imágenes al formato esperado de InceptionV3:\n",
        "\n",
        "* Cambia el tamaño de la imagen a 299px by 299px\n",
        "* [Preprocesa las imágenes](https://cloud.google.com/tpu/docs/inception-v3-advanced#preprocessing_stage) usando el [preprocess_input](https://www.tensorflow.org/api_docs/python/tf/keras/applications/inception_v3/preprocess_input) método para normalizar la imagen de modo que contenga píxeles en el rango de -1 a 1, que coincide con el formato de las imágenes utilizadas para entrenar a InceptionV3.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zXR0217aRPFR",
        "colab": {}
      },
      "source": [
        "def load_image(image_path):\n",
        "    img = tf.io.read_file(image_path)\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, (299, 299))\n",
        "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
        "    return img, image_path"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MDvIu4sXRPFV"
      },
      "source": [
        "## Inicializa InceptionV3 y carga los pesos de Imagenet previamente entrenados.\n",
        "\n",
        "\n",
        "Ahora crearás un modelo tf.keras donde la capa de salida es la última capa convolucional en la arquitectura InceptionV3. La forma de la salida de esta capa es '' 8 x 8 x 2048 ''. Emplea la última capa convolucional porque está utilizando la atención en este ejemplo. No realiza esta inicialización durante el entrenamiento porque podría convertirse en un cuello de botella.\n",
        "\n",
        "* Reenvíarás cada imagen a través de la red y almacenarás el vector resultante en un diccionario (nombre_imagen -> elemento_vector).\n",
        "* Después de que todas las imágenes pasen a través de la red, seleccionas el diccionario y lo guardas en el disco.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RD3vW4SsRPFW",
        "colab": {}
      },
      "source": [
        "image_model = tf.keras.applications.InceptionV3(include_top=False,\n",
        "                                                weights='imagenet')\n",
        "new_input = image_model.input\n",
        "hidden_layer = image_model.layers[-1].output\n",
        "\n",
        "image_features_extract_model = tf.keras.Model(new_input, hidden_layer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rERqlR3WRPGO"
      },
      "source": [
        "## Almacenamiento en caché de las características extraídas de InceptionV3\n",
        "\n",
        "Preprocesarás cada imagen con InceptionV3 y almacenarás en caché la salida en el disco. El almacenamiento en caché de la salida en RAM sería más rápido, pero también requeriría mucha memoria, y requeriría 8 \\ * 8 \\ * 2048 floats por imagen. Al momento de escribir este notebook, esto excede las limitaciones de memoria de Colab (actualmente 12GB de memoria).\n",
        "\n",
        "El rendimiento podría mejorarse con una estrategia de almacenamiento en caché más sofisticada (por ejemplo, fragmentando las imágenes para reducir la I/O del disco de acceso aleatorio), pero eso requeriría más código.\n",
        "\n",
        "El almacenamiento en caché tardará unos 10 minutos en ejecutarse en Colab con una GPU. Si deseas ver una barra de progreso, puedes:\n",
        "\n",
        "1. instalar [tqdm](https://github.com/tqdm/tqdm):\n",
        "\n",
        "    `!pip install -q tqdm`\n",
        "\n",
        "2. Importar tqdm:\n",
        "\n",
        "    `from tqdm import tqdm`\n",
        "\n",
        "3. Cambiar la siguiente línea de código:\n",
        "\n",
        "    `for img, path in image_dataset:`\n",
        "\n",
        "    to:\n",
        "\n",
        "    `for img, path in tqdm(image_dataset):`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Dx_fvbVgRPGQ",
        "colab": {}
      },
      "source": [
        "# Obtenemos imágenes únicas\n",
        "\n",
        "encode_train = sorted(set(img_name_vector))\n",
        "\n",
        "# Siéntete libre de cambiar el batch_size según la configuración de tu sistema\n",
        "image_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\n",
        "image_dataset = image_dataset.map(\n",
        "  load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(16)\n",
        "\n",
        "for img, path in image_dataset:\n",
        "  batch_features = image_features_extract_model(img)\n",
        "  batch_features = tf.reshape(batch_features,\n",
        "                              (batch_features.shape[0], -1, batch_features.shape[3]))\n",
        "\n",
        "  for bf, p in zip(batch_features, path):\n",
        "    path_of_feature = p.numpy().decode(\"utf-8\")\n",
        "    np.save(path_of_feature, bf.numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nyqH3zFwRPFi"
      },
      "source": [
        "## Preprocesar y tokenizar los subtítulos (captions)\n",
        "\n",
        "* Primero, tokenizarás los subtítulos (por ejemplo, dividiendo en espacios). Esto nos da un vocabulario de todas las palabras únicas en los datos (por ejemplo, \"surf\", \"fútbol\", etc.).\n",
        "* A continuación, limitarás el tamaño del vocabulario a las 5,000 palabras principales (para ahorrar memoria). Reemplazarás todas las demás palabras con el token \"UNK\" (desconocido).\n",
        "* Luego creas asignaciones de 'palabra a índice' e 'índice a palabra'.\n",
        "* Y finalmente, rellenas todas las secuencias para que tengan la misma longitud que la más larga."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HZfK8RhQRPFj",
        "colab": {}
      },
      "source": [
        "# Encuentra la longitud máxima de cualquier título en nuestro conjunto de datos\n",
        "def calc_max_length(tensor):\n",
        "    return max(len(t) for t in tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oJGE34aiRPFo",
        "colab": {}
      },
      "source": [
        "# Elige las 5000 palabras principales del vocabulario\n",
        "top_k = 5000\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n",
        "                                                  oov_token=\"<unk>\",\n",
        "                                                  filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
        "tokenizer.fit_on_texts(train_captions)\n",
        "train_seqs = tokenizer.texts_to_sequences(train_captions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8Q44tNQVRPFt",
        "colab": {}
      },
      "source": [
        "tokenizer.word_index['<pad>'] = 0\n",
        "tokenizer.index_word[0] = '<pad>'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0fpJb5ojRPFv",
        "colab": {}
      },
      "source": [
        "# Crea los vectores tokenizados\n",
        "train_seqs = tokenizer.texts_to_sequences(train_captions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AidglIZVRPF4",
        "colab": {}
      },
      "source": [
        "# Rellena cada vector hasta la longitud máxima de los subtítulos\n",
        "# Si no proporcionas un valor de max_length, pad_sequences lo calcula automáticamente\n",
        "cap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gL0wkttkRPGA",
        "colab": {}
      },
      "source": [
        "# Calcula max_length, que se usa para almacenar los pesos de atención\n",
        "max_length = calc_max_length(train_seqs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "M3CD75nDpvTI"
      },
      "source": [
        "## Dividimos los datos en training y testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iS7DDMszRPGF",
        "colab": {}
      },
      "source": [
        "# Creamos los conjuntos de entrenamiento y validación usando una división 80-20\n",
        "img_name_train, img_name_val, cap_train, cap_val = train_test_split(img_name_vector,\n",
        "                                                                    cap_vector,\n",
        "                                                                    test_size=0.2,\n",
        "                                                                    random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XmViPkRFRPGH",
        "colab": {}
      },
      "source": [
        "len(img_name_train), len(cap_train), len(img_name_val), len(cap_val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uEWM9xrYcg45"
      },
      "source": [
        "## Creamos un dataset tf.data para el entrenamiento\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "horagNvhhZiy"
      },
      "source": [
        "¡Nuestras imágenes y subtítulos están listos! A continuación, creemos un conjunto de datos tf.data para usar y entrenar nuestro modelo.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Q3TnZ1ToRPGV",
        "colab": {}
      },
      "source": [
        "# No dudes en cambiar estos parámetros de acuerdo con la configuración de tu sistema\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 1000\n",
        "embedding_dim = 256\n",
        "units = 512\n",
        "vocab_size = top_k + 1\n",
        "num_steps = len(img_name_train) // BATCH_SIZE\n",
        "# Shape of the vector extracted from InceptionV3 is (64, 2048)\n",
        "# These two variables represent that vector shape\n",
        "features_shape = 2048\n",
        "attention_features_shape = 64"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SmZS2N0bXG3T",
        "colab": {}
      },
      "source": [
        "# Cargamos los archivos numpy\n",
        "def map_func(img_name, cap):\n",
        "  img_tensor = np.load(img_name.decode('utf-8')+'.npy')\n",
        "  return img_tensor, cap"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FDF_Nm3tRPGZ",
        "colab": {}
      },
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train))\n",
        "\n",
        "# Usa el mapa para cargar los archivos numpy en paralelo\n",
        "dataset = dataset.map(lambda item1, item2: tf.numpy_function(\n",
        "          map_func, [item1, item2], [tf.float32, tf.int32]),\n",
        "          num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "# Shuffle y batch\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nrvoDphgRPGd"
      },
      "source": [
        "## Modelo\n",
        "\n",
        "Dato curioso: el decodificador a continuación es idéntico al del ejemplo de [Neural Machine Translation with Attention](../sequences/nmt_with_attention.ipynb).\n",
        "\n",
        "La arquitectura del modelo está inspirada en el artículo [Show, Attend and Tell](https://arxiv.org/pdf/1502.03044.pdf).\n",
        "\n",
        "* En este ejemplo, extraemos las características de la capa convolucional inferior de Inception V3 dándonos un vector de forma (8, 8, 2048).\n",
        "* Haremos un reshape a (64, 2048).\n",
        "* Este vector se pasa a través del codificador CNN (que consiste en una sola capa totalmente conectada).\n",
        "* El RNN (aquí GRU) atiende la imagen para predecir la siguiente palabra."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ja2LFTMSdeV3",
        "colab": {}
      },
      "source": [
        "class BahdanauAttention(tf.keras.Model):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, features, hidden):\n",
        "    # características(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n",
        "\n",
        "    # hidden shape == (batch_size, hidden_size)\n",
        "    # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n",
        "    hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
        "\n",
        "    # score shape == (batch_size, 64, hidden_size)\n",
        "    score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
        "\n",
        "    # attention_weights shape == (batch_size, 64, 1)\n",
        "    # you get 1 at the last axis because you are applying score to self.V\n",
        "    attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * features\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AZ7R1RxHRPGf",
        "colab": {}
      },
      "source": [
        "class CNN_Encoder(tf.keras.Model):\n",
        "    # Ya que ya se han extraido las características y las descargamos usando pickle\n",
        "    # Este codificador pasa esas características a través de una capa totalmente conectada\n",
        "    def __init__(self, embedding_dim):\n",
        "        super(CNN_Encoder, self).__init__()\n",
        "        # shape after fc == (batch_size, 64, embedding_dim)\n",
        "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.fc(x)\n",
        "        x = tf.nn.relu(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "V9UbGQmERPGi",
        "colab": {}
      },
      "source": [
        "class RNN_Decoder(tf.keras.Model):\n",
        "  def __init__(self, embedding_dim, units, vocab_size):\n",
        "    super(RNN_Decoder, self).__init__()\n",
        "    self.units = units\n",
        "\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc1 = tf.keras.layers.Dense(self.units)\n",
        "    self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    self.attention = BahdanauAttention(self.units)\n",
        "\n",
        "  def call(self, x, features, hidden):\n",
        "    # definimos la atención como un modelo separado\n",
        "    context_vector, attention_weights = self.attention(features, hidden)\n",
        "\n",
        "    # x shape después de pasar por la incrustación == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # x shape después de la concatenación == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "    # Pasamos el vector concatenado al GRU \n",
        "    output, state = self.gru(x)\n",
        "\n",
        "    # shape == (batch_size, max_length, hidden_size)\n",
        "    x = self.fc1(output)\n",
        "\n",
        "    # x shape == (batch_size * max_length, hidden_size)\n",
        "    x = tf.reshape(x, (-1, x.shape[2]))\n",
        "\n",
        "    # output shape == (batch_size * max_length, vocab)\n",
        "    x = self.fc2(x)\n",
        "\n",
        "    return x, state, attention_weights\n",
        "\n",
        "  def reset_state(self, batch_size):\n",
        "    return tf.zeros((batch_size, self.units))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Qs_Sr03wRPGk",
        "colab": {}
      },
      "source": [
        "encoder = CNN_Encoder(embedding_dim)\n",
        "decoder = RNN_Decoder(embedding_dim, units, vocab_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-bYN7xA0RPGl",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6A3Ni64joyab"
      },
      "source": [
        "## Checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PpJAqPMWo0uE",
        "colab": {}
      },
      "source": [
        "checkpoint_path = \"./checkpoints/train\"\n",
        "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
        "                           decoder=decoder,\n",
        "                           optimizer = optimizer)\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fUkbqhc_uObw",
        "colab": {}
      },
      "source": [
        "start_epoch = 0\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "  start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
        "\n",
        "  # restauramos el último punto de control en checkpoint_path\n",
        "  ckpt.restore(ckpt_manager.latest_checkpoint)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PHod7t72RPGn"
      },
      "source": [
        "## Entrenamiento\n",
        "\n",
        "* Extrae las características almacenadas en los respectivos archivos `.npy` y luego pasa esas características a través del codificador.\n",
        "* La salida del codificador, el estado oculto (inicializado a 0) y la entrada del decodificador (que es el token de inicio) se pasan al decodificador.\n",
        "* El decodificador devuelve las predicciones y el estado oculto del decodificador.\n",
        "* El estado oculto del decodificador se devuelve al modelo y las predicciones se utilizan para calcular la pérdida.\n",
        "* Usará un 'forzamiento maestro' para decidir la próxima entrada al decodificador.\n",
        "* El forzamiento maestro es la técnica en la que se pasa la palabra objetivo como la siguiente entrada al decodificador.\n",
        "* El paso final es calcular los gradientes y aplicarlos al optimizador y la retropropagación.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Vt4WZ5mhJE-E",
        "colab": {}
      },
      "source": [
        "# agregamos esto en una celda separada porque si ejecuta la celda de entrenamiento\n",
        "# muchas veces, la matriz loss_plot se restablecerá\n",
        "loss_plot = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sqgyz2ANKlpU",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(img_tensor, target):\n",
        "  loss = 0\n",
        "\n",
        "  # inicializando el estado oculto para cada batch\n",
        "  # porque los subtítulos no están relacionados de una imagen a otra\n",
        "  hidden = decoder.reset_state(batch_size=target.shape[0])\n",
        "\n",
        "  dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "      features = encoder(img_tensor)\n",
        "\n",
        "      for i in range(1, target.shape[1]):\n",
        "          # Pasamos las características a través del decodificador\n",
        "          predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
        "\n",
        "          loss += loss_function(target[:, i], predictions)\n",
        "\n",
        "          # usamos el teacher forcing (\"forzamiento maestro\")\n",
        "          dec_input = tf.expand_dims(target[:, i], 1)\n",
        "\n",
        "  total_loss = (loss / int(target.shape[1]))\n",
        "\n",
        "  trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, trainable_variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
        "\n",
        "  return loss, total_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UlA4VIQpRPGo",
        "colab": {}
      },
      "source": [
        "EPOCHS = 20\n",
        "\n",
        "for epoch in range(start_epoch, EPOCHS):\n",
        "    start = time.time()\n",
        "    total_loss = 0\n",
        "\n",
        "    for (batch, (img_tensor, target)) in enumerate(dataset):\n",
        "        batch_loss, t_loss = train_step(img_tensor, target)\n",
        "        total_loss += t_loss\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            print ('Epoch {} Batch {} Loss {:.4f}'.format(\n",
        "              epoch + 1, batch, batch_loss.numpy() / int(target.shape[1])))\n",
        "    # almacenamos el valor de pérdida final de las épocas para trazarlo más tarde\n",
        "    loss_plot.append(total_loss / num_steps)\n",
        "\n",
        "    if epoch % 5 == 0:\n",
        "      ckpt_manager.save()\n",
        "\n",
        "    print ('Epoch {} Loss {:.6f}'.format(epoch + 1,\n",
        "                                         total_loss/num_steps))\n",
        "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1Wm83G-ZBPcC",
        "colab": {}
      },
      "source": [
        "plt.plot(loss_plot)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss Plot')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xGvOcLQKghXN"
      },
      "source": [
        "## ¡Subtítulos!\n",
        "\n",
        "* La función de evaluación es similar al ciclo de entrenamiento, excepto que no utiliza el forzamiento maestro aquí. La entrada al decodificador en cada intervalo de tiempo son sus predicciones anteriores junto con el estado oculto y la salida del codificador.\n",
        "* Deja de predecir cuando el modelo predice el token final.\n",
        "* Y almacena los pesos de atención para cada intervalo de tiempo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RCWpDtyNRPGs",
        "colab": {}
      },
      "source": [
        "def evaluate(image):\n",
        "    attention_plot = np.zeros((max_length, attention_features_shape))\n",
        "\n",
        "    hidden = decoder.reset_state(batch_size=1)\n",
        "\n",
        "    temp_input = tf.expand_dims(load_image(image)[0], 0)\n",
        "    img_tensor_val = image_features_extract_model(temp_input)\n",
        "    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n",
        "\n",
        "    features = encoder(img_tensor_val)\n",
        "\n",
        "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
        "    result = []\n",
        "\n",
        "    for i in range(max_length):\n",
        "        predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n",
        "\n",
        "        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n",
        "\n",
        "        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n",
        "        result.append(tokenizer.index_word[predicted_id])\n",
        "\n",
        "        if tokenizer.index_word[predicted_id] == '<end>':\n",
        "            return result, attention_plot\n",
        "\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    attention_plot = attention_plot[:len(result), :]\n",
        "    return result, attention_plot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fD_y7PD6RPGt",
        "colab": {}
      },
      "source": [
        "def plot_attention(image, result, attention_plot):\n",
        "    temp_image = np.array(Image.open(image))\n",
        "\n",
        "    fig = plt.figure(figsize=(10, 10))\n",
        "\n",
        "    len_result = len(result)\n",
        "    for l in range(len_result):\n",
        "        temp_att = np.resize(attention_plot[l], (8, 8))\n",
        "        ax = fig.add_subplot(len_result//2, len_result//2, l+1)\n",
        "        ax.set_title(result[l])\n",
        "        img = ax.imshow(temp_image)\n",
        "        ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7x8RiPHe_4qI",
        "colab": {}
      },
      "source": [
        "# Subtítulos en el set de validación\n",
        "rid = np.random.randint(0, len(img_name_val))\n",
        "image = img_name_val[rid]\n",
        "real_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]])\n",
        "result, attention_plot = evaluate(image)\n",
        "\n",
        "print ('Real Caption:', real_caption)\n",
        "print ('Prediction Caption:', ' '.join(result))\n",
        "plot_attention(image, result, attention_plot)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Rprk3HEvZuxb"
      },
      "source": [
        "## Inténtalo con tus propias imágenes\n",
        "\n",
        "Por diversión, a continuación proporcionamos un método que puedes usar para subtitular tus propias imágenes con el modelo que acabamos de entrenar. Ten en cuenta que fue entrenado con una cantidad relativamente pequeña de datos, y tus imágenes pueden ser diferentes de los datos de entrenamiento (¡así que prepárate para obtener resultados extraños!)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9Psd1quzaAWg",
        "colab": {}
      },
      "source": [
        "image_url = 'https://tensorflow.org/images/surf.jpg'\n",
        "image_extension = image_url[-4:]\n",
        "image_path = tf.keras.utils.get_file('image'+image_extension,\n",
        "                                     origin=image_url)\n",
        "\n",
        "result, attention_plot = evaluate(image_path)\n",
        "print ('Prediction Caption:', ' '.join(result))\n",
        "plot_attention(image_path, result, attention_plot)\n",
        "# Abrimos la imagen\n",
        "Image.open(image_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VJZXyJco6uLO"
      },
      "source": [
        "# Siguientes pasos\n",
        "\n",
        "¡Felicidades! Acabas de entrenar un modelo de subtítulos de imágenes con atención. A continuación, echa un vistazo a este ejemplo.\n",
        "[Neural Machine Translation with Attention](../sequences/nmt_with_attention.ipynb). Utiliza una arquitectura similar para traducir oraciones en español e inglés. También puedes experimentar entrenando el código en este cuaderno en un conjunto de datos diferente."
      ]
    }
  ]
}