{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IRS.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dISYzX4ob2l7",
        "colab_type": "text"
      },
      "source": [
        "# PEC 2. Introducción a los sistemas de recuperación de información.\n",
        "\n",
        "Vamos a desarrollar un sistema de recuperación de información básico. Partiendo de una lista de documentos de texto tendrás que usar las técnicas de recuperación para obtener, procesar y analizar datos útiles a partir de texto.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oio_-Jm3K_kJ",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# **Parte 1**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8HTHe0enNTp",
        "colab_type": "text"
      },
      "source": [
        "## 1. Representación de documentos\n",
        "\n",
        "Supongamos que tenemos una colección con los siguientes 5 documentos. Una forma de gestionar una lista de documentos y hacerla más fácilmente accesible es mediante un tipo de datos nativo en Python denominados diccionarios. Los **diccionarios en Python** son un tipo de estructuras de datos que permiten guardar un conjunto no ordenado de pares clave-valor, donde las claves son únicas, es decir, que no pueden existir dos elementos con una misma clave. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQmiXX4K-Vo3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "document_1 = \"I love watching movies when it's cold outside ;-)\"\n",
        "document_2 = \"Toy Story is the best animation movie ever, I love it!\"\n",
        "document_3 = \"Watching horror movies alone at night is really scary\"\n",
        "document_4 = \"He loves to watch films filled with suspense and unexpected plot twists\"\n",
        "document_5 = \"My mom loves to watch movies. My dad hates movie theaters. My brothers like any kind of movie. And I haven't watched a single movie since I got into college\"\n",
        "documents = [document_1, document_2, document_3, document_4, document_5]\n",
        "\n",
        "docIds = ['doc01','doc02','doc03','doc04','doc05']\n",
        "\n",
        "documents_dict =  # TODO : generar un diccionario de la forma { 'doc01': 'I love..', ... }\n",
        "\n",
        "print ( documents_dict['doc01'] )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyEBRDWH_bl9",
        "colab_type": "text"
      },
      "source": [
        "### 1.1 Separando las palabras\n",
        "\n",
        "Lo primero que hay que hacer para poder tratar el texto es separar las palabras que lo componen, es decir, hacer una lista  de palabras. El modelo que vamos a utilizar aquí se denomina [**bolsa-de-palabras**](https://es.wikipedia.org/wiki/Modelo_bolsa_de_palabras) (bag-of-words) ya que nos interesan las palabras sin importar su posición o importancia en el documento.  \n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-egvcmLi_zYb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "documents_words = [########## for doc in documents_dict.values()]  # TODO : separarar las palabras con split para hacer una lista de palabras\n",
        "documents_words_dict = dict ( zip ( documents_dict.keys(), documents_words )) # Obtener un diccionario del tipo { 'doc01': ['I', 'love', ...],... }\n",
        "\n",
        "print ( documents_words_dict['doc01'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dcog4kowxtR",
        "colab_type": "text"
      },
      "source": [
        "El problema es que separar las palabras no es una tarea trivial. No solo el espacio en blanco actúa de separador, también lo hace el punto (``.``), la coma (``,``), los paréntesis (``(),[],{}``), la interrogación (``?``), la admiración (``!``), etc. Aunque estos signos de puntuación tienen múltiples funciones. Así el punto también sirve para separar los acrónimos (siglas, como EE.UU.) o partes de los números (como 1.024,64).\n",
        "\n",
        "\n",
        "Por lo tanto, vamos a utilizar [**expresiones regulares**](https://es.wikipedia.org/wiki/Expresi%C3%B3n_regular) para separar las palabras. Consideramos como un palabra una combinación del caracteres alfabéticos separado por cualquier otro carácter.\n",
        "\n",
        "\n",
        "Para ello vamos a utilizar la librería [NTLK](https://es.wikipedia.org/wiki/NLTK) (Natural Language Toolkit) y dentro de ella la función [RegexpTokenizer](https://www.nltk.org/_modules/nltk/tokenize/regexp.html) que divide una cadena de texto usando expresiones regulares.\n",
        "\n",
        "De todas los token obtenidos vamos a desechar lo que no sean palabras (conjuntos de caracteres alfabéticos): números, signos de puntuación, etc \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuTRbIIbz4ur",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "def tokenizer (text):\n",
        "  tokens = RegexpTokenizer()  # TODO : separar palabras\n",
        "  alpha = [ ] # TODO : eliminar tokens que no son alfabeticos \n",
        "  return alpha\n",
        "\n",
        "documents_words = [##########  for doc in documents_dict.values()]  # TODO\n",
        "documents_words_dict = dict ( zip ( documents_dict.keys(), documents_words ))\n",
        "print(documents_words_dict['doc02'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdI5WbsUSLxB",
        "colab_type": "text"
      },
      "source": [
        "**Q1 - Al separar las palabras por el método de las expresiones regulares, ¿notas algún problema o posible mejora?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGHdMG7i2QLm",
        "colab_type": "text"
      },
      "source": [
        "### 1.2 Convirtiendo a minúsculas\n",
        "\n",
        "Una estrategia para reducir el número de palabras es convertirlas a minúsculas, pues algunos signos de puntuación modifican la letra inicial de las palabras. Así se consigue reducir el número de variantes de una misma palabra.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rApv0Qm26__",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lowerize ( tokens ):\n",
        "  lower_tokens = [] # TODO : convertir a minúsculas\n",
        "  return lower_tokens\n",
        "\n",
        "documents_words_lower = [ ######## for tokens in documents_words_dict.values() ] # TODO : convertir a minusculas todas los documentos\n",
        "\n",
        "documents_words_lower_dict = dict ( zip ( documents_dict.keys(),documents_words_lower))\n",
        "print (documents_words_lower_dict ['doc02'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UHTmwfKhjXK",
        "colab_type": "text"
      },
      "source": [
        "**Q2 - ¿Qué alternativas propones a la conversión de todas las palabras a minúsculas?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BjjkQTa4DtX",
        "colab_type": "text"
      },
      "source": [
        "### 1.3 Palabras vacías\n",
        "\n",
        "Las palabras vacías (stopwords) son palabras con sentido gramatical pero con poco significado. Además estas palabras que incluyen artículos, preposiciones, conjunciones, pronombres, etc. son muy frecuentes en los documentos. Así que su eliminación reduce considerablemente el número de palabras.\n",
        "\n",
        "NLTK tiene listas de palabras vacías en 16 idiomas. En este caso, se ha cargado la lista en inglés."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEZzmCrT6kff",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "def stop_words_remover ( tokens ):\n",
        "  stopwordlist = stopwords.words ('english')\n",
        "  #print (stopwordlist)\n",
        "  stopped = [  ] # TODO : eliminar de la lista las palabras vacías (stopwordlist)\n",
        "  return stopped\n",
        "\n",
        "documents_words_stopped = [ ######## for tokens in documents_words_lower_dict.values()] # TODO: eliminar las palabras vacias de los documentos\n",
        "documents_words_stopped_dict = dict ( zip ( documents_dict.keys(),documents_words_stopped))\n",
        "print (documents_words_stopped_dict['doc02'])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7scSzTt8jLl",
        "colab_type": "text"
      },
      "source": [
        "### 1.4 Normalización \n",
        "\n",
        "Muchos idiomas contienen palabras derivadas de otras y esto se denomina [flexión](https://es.wikipedia.org/wiki/Flexi%C3%B3n_(ling%C3%BC%C3%ADstica)). La flexión es la modificación de una para expresar diferentes categorias gramaticales como persona, número, género, etc.\n",
        "\n",
        "Tratar esta flexión para llevar las palabras a una forma base se denomina **normalización de palabras**. La normalización permite que si se busca por una palabra se haga al mismo tiempo por todas sus flexiones.\n",
        "\n",
        "La **lematización** es el proceso de reducir la inflexión de las palabras para llevarla a su forma origen o raíz. El lema es la parte de la palabra a la que se añade la flexión.\n",
        "\n",
        "En NLTK hay disponibles diferentes lematizadores aunque aquí vamos a utilizar el más conocido: el [algoritmo de Porter](https://es.wikipedia.org/wiki/Algoritmo_de_Porter). \n",
        "\n",
        "\n",
        "Podéis ver algo más de estos procesos en \n",
        "https://www.datacamp.com/community/tutorials/stemming-lemmatization-python\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jeHnYNnfSuUg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "def stemmer ( tokens ):\n",
        "  ps = PorterStemmer()\n",
        "  stemmed = [ ] # TODO: usar ps.stem para poner todos los token en forma base\n",
        "  return stemmed\n",
        "\n",
        "documents_words_stemmed = [ ######### for tokens in documents_words_stopped_dict.values()] # TODO : lematizar todos los documentos\n",
        "documents_words_stemmed_dict = dict ( zip ( documents_dict.keys(),documents_words_stemmed))\n",
        "print (documents_words_stemmed_dict ['doc02'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VV2_ZiodeHEC",
        "colab_type": "text"
      },
      "source": [
        "### 1.5 Dando peso a las palabras: ponderación mediante IF*IDF\n",
        "\n",
        "Ahora vamos a ver cómo de importante es una palabra/término en los documentos. Para ello podemos calcular la frecuencia de cada término contando el número de veces que aparece en cada documento, que será una medida de su peso o importancia.\n",
        "\n",
        "$TF (t,d) = f_{t,d}$  (#número de repeticiones del término $t$ en el documento $d$)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ir0o8K4jeskl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def termsFrequency ( terms ):\n",
        "    freq = {} \n",
        "    for                           #TODO : para cada término del documento\n",
        "                                  #TODO : añadir el termino al diccionario freq y calcular su frecuencia en el documento \n",
        "                                  #TODO\n",
        "    return freq\n",
        "\n",
        "\n",
        "\n",
        "freqs = [ ######### for terms in documents_words_stemmed_dict.values() ] # TODO: calcular las frecuencias los terminos de cada documento\n",
        "freq_dict = dict ( zip ( documents_dict.keys(), freqs))\n",
        "print ( freq_dict['doc05'] )\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENBdvpVvmgjT",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "Para penalizar a los documentos más largos (con más palabras y mayor frecuencia de las mismas) se suele usar la **frecuencia normalizada**, es decir, el cociente entre la frecuencia del término y el número total de palabras del documento:\n",
        "\n",
        "$TF(t,d ) = \\frac{f_{t,d}}{\\sum_{u \\in d} f_{u,d}} $\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48RUdNijq7kq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def termsFrequencyNormalized ( freqs ):\n",
        "  freq_n = {}\n",
        "  sumfreq =                         # TODO : calcular el numero de terminos en el documento\n",
        "  for                               # TODO : para cada termino\n",
        "    freq_n[    ] =                  # TODO : calcular su frecuencia normalizada\n",
        "  return freq_n\n",
        "\n",
        "freq_dict_n =  [#########  for terms   in freq_dict.values() ] # TODO : calcular las frecuencias normalizadas\n",
        "freq_dict_norm =  dict ( zip ( documents_dict.keys(),freq_dict_n))\n",
        "print (freq_dict_norm['doc05'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVAUfXfnsTYg",
        "colab_type": "text"
      },
      "source": [
        "La **frecuencia inversa de documentos** para un término $t$ es el logaritmo (generalemente en base 2 aunque puede ser cualquier base) del cociente entre el número de documentos y el número de documentos en los que aparece el término $t$. \n",
        "\n",
        "$ IDF (t) = log_2 \\frac{N}{\\{d \\in D : t \\in d \\}} $\n",
        "\n",
        "A mayor puntuación de TF*IDF el término es más específico y a menor puntuación, más genérico."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_EpmVI5wB9X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "\n",
        "def IDF ( doc_freq_dict ):\n",
        "  idf_dict = {}\n",
        "  N =                                # TODO : número de documentos en la colección\n",
        "  \n",
        "  for                                # TODO: calcular el número de documentos en los que aparece cada término\n",
        "                                     # TODO\n",
        "  \n",
        "  for                                # TODO : calcular la frecuencia inversa de documento de cada término\n",
        "    idf_dict[ ] =                    # TODO\n",
        "    \n",
        "  return idf_dict\n",
        "\n",
        "idfs = IDF ( freq_dict_norm )\n",
        "print (idfs)  # resultado esperado : {'love': 0.32192809488736235, ... }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfOhITkjb8wK",
        "colab_type": "text"
      },
      "source": [
        "### 1.6 De palabras a términos\n",
        "\n",
        "Ahora ya tenemos las funciones necesarias para procesar el texto y para la ponderación.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqBpO80hox42",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def processText (text):\n",
        "    tokens =                 # TODO : separar las palabras  \n",
        "    lower_tokens =           # TODO : convertir a minusculas\n",
        "    stopped =                # TODO : eliminar palabras vacias \n",
        "    stemmed =                # TODO : lematizar las palabras\n",
        "    return stemmed\n",
        "\n",
        "tf = [####### for doc in documents_dict.values()] #TODO : calcular la frecuency normalizada de las palabras procesadas\n",
        "tf_dict = dict ( zip ( documents_dict.keys(), tf)) # diccionario {'doc01': {'love': 0.2, 'watch': 0.2, ...}...}\n",
        "\n",
        "idf =       # TODO : calcular la frecuencia inversa de documento (a partir de tf_dict)\n",
        "\n",
        "print (tf_dict)\n",
        "print (idf)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fBkZF380N2p",
        "colab_type": "text"
      },
      "source": [
        "## 2. Construyendo el espacio vectorial\n",
        "\n",
        "Usando las funciones anteriores vamos a construir la matriz de documentos (en filas) y términos (en columnas). Para facilitar la tarea usaremos una estructura de datos ya conocida, el **dataframe**. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZ0Cf75k0SJa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "def generateTF_IDF_Table ( tf_dict , idfs):\n",
        "  uniqueTerms = [  ] # TODO : obtener los términos (únicos) a partir de los diccionario de frecuencias\n",
        "  uniqueTerms.sort()\n",
        "  table = []\n",
        "  for freqs                # TODO : recorrer todas las frecuecias de los documentos\n",
        "      d = dict.fromkeys(uniqueTerms, 0.0)\n",
        "      for t in freqs:\n",
        "        d [ t ] =          # TODO : calcular el producto tf * idf para cada termino \n",
        "      table.append ( d )\n",
        "  df = pd.DataFrame ( table , index = tf_dict.keys())\n",
        "  return df\n",
        " \n",
        "df = generateTF_IDF_Table (tf_dict, idf)\n",
        "df.head(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XGQ7247cB1i",
        "colab_type": "text"
      },
      "source": [
        "### 2.1 Qué documentos son más parecidos (o tratan de lo mismo)\n",
        "\n",
        "Ahora vamos a ver qué documentos se parecen más entre sí mediante la técnica de la similitud del coseno. \n",
        "\n",
        "Para ello vamos a utilizar la librería [sklearn](https://scikit-learn.org/stable/), aunque sólo la funcionalidad para calcular la similitud del coseno. \n",
        "\n",
        "Como resultado obtendremos una matriz y una valor del coseno para pares de documentos como medida de lo similares que son. Evidentemente, cada documento será lo más parecido posible consigo mismo. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmU9E-sMG3uO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "cosine_similarity_matrix =  #TODO : calcular la similitud del coseno entre pares de documentos\n",
        "#print(cosine_similarity_matrix)\n",
        "\n",
        "\n",
        "# TODO Calcular los 2 documentos más similares según la similitud del coseno (y que no sea consigo mismo)\n",
        "np.fill_diagonal(cosine_similarity_matrix, 0.0) # poner la diagonal principal a ceros\n",
        "ind = np.unravel_index ( ######### , cosine_similarity_matrix.shape) # calcular la coordenadas del máximo\n",
        "print (ind)\n",
        "print ( cosine_similarity_matrix[ind])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqmI2hETN6mo",
        "colab_type": "text"
      },
      "source": [
        "Representando gráficamente esta similitud entre documentos:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyTVnTyWxIhz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "from matplotlib import cm\n",
        "\n",
        "A = cosine_similarity_matrix.flatten().reshape (5,5)\n",
        "plt.imshow(A, cmap='viridis', interpolation='nearest')\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aW8U52QvOH3Z",
        "colab_type": "text"
      },
      "source": [
        "### 2.2 Resolviendo consultas\n",
        "\n",
        "Vamos a resolver consultas (obtener los documentos más relevantes) considerando la consulta como un vector y comparándolo con el conjunto de documentos mediante la similitud del coseno. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rS3840-66HSl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "q = \"I watched alone a horror film\"\n",
        "\n",
        "q_mod = processText (q)\n",
        "\n",
        "def generateQueryTable( query ):\n",
        "  df_query = pd.DataFrame ( data = None, columns = df.columns , index=['q'] )\n",
        "  for x in df.columns:\n",
        "    if x in query:\n",
        "      df_query[x]=1.0\n",
        "    else:\n",
        "      df_query[x]=0.0\n",
        "  \n",
        "  return df_query  \n",
        "\n",
        "df_query = generateQueryTable (q_mod)\n",
        "df_query.head(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOB99AC680Je",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "%time q_cossim =  #TODO : calcular la similitud del coseno e indicar qué documento es el más próximo a la consulta\n",
        "\n",
        "ind = np.unravel_index ( np.argmax(q_cossim, axis=None), q_cossim.shape)\n",
        "print (documents[ind[0]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acGZF4CID0JD",
        "colab_type": "text"
      },
      "source": [
        "## 2. Construyendo un índice invertido\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDPRfbhvoVy6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generateInvertedIndex (tf_dict , idfs):\n",
        "  uniqueTerms = [ ] # TODO : terminos únicos\n",
        "  #print('Vocabulary = ', len(uniqueTerms)) #no. of unique words\n",
        "  inv_index = {} #dictionary: key =word/term, value: [docid, termfrequency * inversedocumentfrecuency]\n",
        "\n",
        "  for vocab                 # TODO : construir el índice invertido como diccionario\n",
        "                             # TODO\n",
        "\n",
        "  return inv_index \n",
        "  \n",
        "%time index = generateInvertedIndex (tf_dict, idfs)\n",
        "%time results =   { x[0] : x[1]  for x  in index['movi'] }\n",
        "\n",
        "print ( results  )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdzRcmXf8Tko",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# evaluar consulta\n",
        "q = \"I watched alone a horror film\"\n",
        "\n",
        "def processQuery ( index,  q ):\n",
        "  results = {}\n",
        "  for : #TODO : para cada termino en la consulta\n",
        "    if :      # TODO : comprobar que está en el índice\n",
        "              # TODO : obtener lista de documentos\n",
        "               # TODO : para cada documento \n",
        "               # TODO : sumar los tf*idf por documento\n",
        "               #TODO \n",
        "\n",
        "\n",
        "  return results\n",
        "      \n",
        "%time r = processQuery( index ,  processText ( q ) )\n",
        "sorted_results =  # TODO : ordenar resultados de mayor a menor\n",
        "#print (sorted_results [0:2])\n",
        "print (documents_dict [ sorted_results [0] ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMhTwFfUYfSu",
        "colab_type": "text"
      },
      "source": [
        "**Q3 - ¿Qué es más eficiente para resolver la consulta : el dataframe o el índice invertido?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usntxOx2fHCH",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# **Parte 2**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zs_wEjGdcU8R",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "![TED Talks](https://storage.googleapis.com/kaggle-datasets-images/2405/4030/bf5c2fd8ab2faec33e8a0451ba674c6a/dataset-cover.jpg)\n",
        "\n",
        "En esta segunda parte vamos a utilizar un caso más real para afianzar los conocimiento adquiridos. Para ello usaremos las transcriciones de las charlas TED, que son conferencias de unos 20 minutos de duración.\n",
        "\n",
        "\n",
        "Vamos a cargas las transcripciones y generar un dataframe para tratarlas.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzExVwmgTh1X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjj20nUB62Rt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import io\n",
        "\n",
        "\n",
        "data_ted = pd.read_csv(io.StringIO(uploaded['transcripts_ted_talks.csv'].decode('utf-8')))\n",
        "#data_ted = pd.read_csv('./transcripts_ted_talks.csv')\n",
        "data_ted.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gantmAJ-Lewh",
        "colab_type": "text"
      },
      "source": [
        "Obteniendo el título de la conferencia a partir de la URL.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzwLLx_6VmzL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_ted['title']= data_ted['url'].map(lambda x:x.split(\"/\")[-1]).str.replace(\"_\",\" \").str.upper().str.strip()\n",
        "data_ted.head(5)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYbZlZrJLzRu",
        "colab_type": "text"
      },
      "source": [
        "Obtener las frecuencias de término por documento y la frecuencia inversa de documento para el texto contenido en el título (title) y en la transcripción (transcript) de la charla."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ll0CX2btW09j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf_ted = [###### for text in data_ted [['title','transcript']].astype(str).apply (' '.join, axis=1) ] #TODO : obtener la frecuencia normalizada de todas las transcripciones\n",
        "tf_ted_dict = dict ( zip ( data_ted ['title'], tf_ted)) # generar el diccionario usando el titulo como clave\n",
        "\n",
        "idf_ted =  # TODO : calcular la frecuencia inversa de documento "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftsA-dizMxMU",
        "colab_type": "text"
      },
      "source": [
        "Generar un dataframe con los documentos como filas y los terminos normalizados como columnas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvjUh0wkZ0v4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ted_df =  # TODO : generar dataframe\n",
        "ted_df.head(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2dGJtNDM_5D",
        "colab_type": "text"
      },
      "source": [
        "**Q4 - ¿Qué dimensiones tiene el dataframe obtenido de procesar las charlas TED?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dn_mft4vNlIy",
        "colab_type": "text"
      },
      "source": [
        "A continuación buscamos un par de charlas que sean muy similares."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kx2Hx11wrM_L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "ted_cosine_similarity =  #TODO : calcular la similitud del coseno entre todos las transcripciones\n",
        "\n",
        "np.fill_diagonal(ted_cosine_similarity, 0.0) \n",
        "ind = np.unravel_index ( ########, ted_cosine_similarity.shape) # TODO:\n",
        "\n",
        "print (ted_df.index [ind[0]])\n",
        "print ( ted_df.index [ind[1]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZS3ictr0Nz1g",
        "colab_type": "text"
      },
      "source": [
        "**Q5 - ¿De qué tratan las 2 charlas más similares entre todas las charlas TED?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70rsntb4Oe07",
        "colab_type": "text"
      },
      "source": [
        "Generar un índice invertido (o utilizar el dataframe) para resolver las siguientes consultas:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UaWVtLgUgL4E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%time ted_inv_index =  # TODO : generar indice invertido  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJsRWDT5hpZO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "q_ted = \"\" # TODO : resolver las charlas que traten de virus y pandemias (virus pandemic)\n",
        "\n",
        "%time res = processQuery  # TODO\n",
        "\n",
        "ted_sorted_results =  # TODO\n",
        "print (ted_sorted_results ) # TODO : los 3 charlas más relevantes sobre virus y pandemias  \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfRcaBe2aRh7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "q_ted = \"\" # TODO : consultar las charlas sobre cambio climático (climate change)\n",
        "\n",
        "%time res = processQuery #TODO\n",
        "\n",
        "ted_sorted_results =  # TODO\n",
        "print (ted_sorted_results [0:9])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6iDXKe0BfFCr",
        "colab_type": "text"
      },
      "source": [
        "**Q6 - ¿Quién es el ponente de la charla más relevante sobre cambio climático?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5NIdS4qGxcf",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# Parte 3. Comprobar la ley Zipf\n",
        "\n",
        "\n",
        "La **ley Zipf** debe su nombre al lingüista norteamericano George Kingsley Zipf y dictamina que un pequeño número de palabras se utilizan todo el tiempo mientras que una gran mayoría de ellas apenas se utiliza. No es muy sorprendente que palabras muy frecuentes en textos en inglés sean `the`, `of` y similares, y que palabras como `intrauterine` apenas se utilicen. [https://en.wikipedia.org/wiki/Zipf%27s_law]\n",
        "\n",
        "La ley Zipf puede escribirse de la siguiente forma: la $r$-ésima palabra más frecuente $f(r)$ escala según la fórmula: $f(r) \\propto \\frac{1}{r^{\\alpha}}$ con $\\alpha \\approx 1$.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMdZ37W6dRQb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "frequency = {}\n",
        "\n",
        "#TODO : calcular las frecuencias de todas las palabras de las charlas (NO eliminar palabras vacías ni lematizar)\n",
        "transcripts = [ ######## for text in data_ted [['title','transcript']].astype(str).apply (' '.join, axis=1) ]\n",
        "\n",
        "for words in transcripts:\n",
        "  for word in words:\n",
        "    count = # TODO\n",
        "    frequency[word] =  #TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GX5GkSucXtR8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from operator import itemgetter   \n",
        "\n",
        "frequency_sorted =  #TODO\n",
        "print (frequency_sorted[0:10])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZI3Ok5XdEdK",
        "colab_type": "text"
      },
      "source": [
        "**Q7 - ¿Cuáles son las 10 palabras más frecuentes en el las charlas TED?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9GH15z5XxWK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def createZipfTable(frequencies):\n",
        "  zipf_table = []\n",
        "  top_frequency =  #TODO\n",
        "  #print ('Frecuencia máxima: ' + str(top_frequency) )\n",
        "\n",
        "  for index, item in enumerate(frequencies , start=1):\n",
        "    relative_frequency = \"1/{}\".format(index)\n",
        "    zipf_frequency = top_frequency * (1 / index)\n",
        "    zipf_table.append({\"word\": item[0], \"actual_frequency\": item[1], \"relative_frequency\": relative_frequency , \"zipf_frequency\": zipf_frequency })\n",
        "\n",
        "  return zipf_table\n",
        "\n",
        "\n",
        "zipf_table = createZipfTable (frequency_sorted[0:10])\n",
        "\n",
        "\n",
        "print(\"|Rank|    Word    |       Freq | Zipf Frac  | Zipf Freq  |\")\n",
        "format_string = \"|{:4}|{:12}|{:12.0f}|{:>12}|{:12.2f}|\"\n",
        "for index, item in enumerate(zipf_table,start=1):\n",
        "        print(format_string.format(index,\n",
        "                                   item[\"word\"],\n",
        "                                   item[\"actual_frequency\"],\n",
        "                                   item[\"relative_frequency\"],\n",
        "                                   item[\"zipf_frequency\"]))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkfYpk-VfY5y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "zipf_table = createZipfTable (frequency_sorted[0:100])\n",
        "\n",
        "ranks = list (range ( 1, 1+len (zipf_table) ))\n",
        "frequencies = [ rec['actual_frequency']  for rec in zipf_table ]\n",
        "zipf_frequencies = [ rec['zipf_frequency'] for rec in zipf_table ] \n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "\n",
        "plt.title(\"Word Frequencies in TED Talks\")\n",
        "plt.ylabel(\"Total Number of Occurrences\")\n",
        "plt.xlabel(\"Rank of words\")\n",
        "\n",
        "plt.plot(\n",
        "    ranks,\n",
        "    frequencies, color='blue', label='Actual freq.',\n",
        "    alpha=0.5\n",
        "  )\n",
        "\n",
        "\n",
        "plt.plot(\n",
        "    ranks,\n",
        "    zipf_frequencies, color=\"orange\", label='Expected Zipf.',linestyle='--',\n",
        "    alpha=0.5\n",
        "  )\n",
        "\n",
        "plt.legend()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIl-vPdMyQvW",
        "colab_type": "text"
      },
      "source": [
        "**Q8 - A la vista de los resultados, ¿consideras que las frecuencias siguen una distribución quasi-zipfianica?**\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8HDV2n_QqFu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}